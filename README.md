📌 Project Title:
Multimodal Sentiment Analysis for Online Interviews

🧠 Project Description:
In the era of remote hiring and virtual interviews, maintaining fairness, integrity, and real-time evaluation of candidates has become a critical challenge. Our project presents a Multimodal Sentiment Analysis System that detects signs of cheating, stress, or dishonesty during online interviews by analyzing audio, video, and text data in parallel.

The system captures:

Verbal cues via speech-to-text transcription using OpenAI's Whisper.

Text-based sentiment from the transcript using DistilBERT.

Facial expressions and emotions through DeepFace, CNN layers, and Haar Cascades.

Non-verbal cues like gaze direction, micro-expressions, and head movement using MediaPipe and OpenCV.

🎯 Key Features:
Real-time detection of emotions, stress levels, and engagement.

Fusion of multiple data streams (audio, video, text) for accurate predictions.

Achieved 93.1% accuracy in test datasets.

Designed for HR departments, remote hiring platforms, and exam proctoring systems.

👩‍💻 Your Role:
Integrated and synchronized all components (audio, video, NLP).

Trained and fine-tuned CNN layers for emotion detection.

Designed and implemented the fusion logic for decision-making.

Built a user interface using Streamlit for easy interaction.

📊 Technologies & Libraries:
Whisper – Speech-to-text

DistilBERT – Text sentiment analysis

DeepFace, Haar Cascade, CNN – Facial emotion detection

MediaPipe, OpenCV – Non-verbal cue detection

Streamlit – Web interface

Torch, TensorFlow, Transformers, NLTK, TextBlob – AI/ML stack
