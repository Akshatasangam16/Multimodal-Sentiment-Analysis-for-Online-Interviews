ğŸ“Œ Project Title:
Multimodal Sentiment Analysis for Online Interviews

ğŸ§  Project Description:
In the era of remote hiring and virtual interviews, maintaining fairness, integrity, and real-time evaluation of candidates has become a critical challenge. Our project presents a Multimodal Sentiment Analysis System that detects signs of cheating, stress, or dishonesty during online interviews by analyzing audio, video, and text data in parallel.

The system captures:

Verbal cues via speech-to-text transcription using OpenAI's Whisper.

Text-based sentiment from the transcript using DistilBERT.

Facial expressions and emotions through DeepFace, CNN layers, and Haar Cascades.

Non-verbal cues like gaze direction, micro-expressions, and head movement using MediaPipe and OpenCV.

ğŸ¯ Key Features:
Real-time detection of emotions, stress levels, and engagement.

Fusion of multiple data streams (audio, video, text) for accurate predictions.

Achieved 93.1% accuracy in test datasets.

Designed for HR departments, remote hiring platforms, and exam proctoring systems.

ğŸ‘©â€ğŸ’» Your Role:
Integrated and synchronized all components (audio, video, NLP).

Trained and fine-tuned CNN layers for emotion detection.

Designed and implemented the fusion logic for decision-making.

Built a user interface using Streamlit for easy interaction.

ğŸ“Š Technologies & Libraries:
Whisper â€“ Speech-to-text

DistilBERT â€“ Text sentiment analysis

DeepFace, Haar Cascade, CNN â€“ Facial emotion detection

MediaPipe, OpenCV â€“ Non-verbal cue detection

Streamlit â€“ Web interface

Torch, TensorFlow, Transformers, NLTK, TextBlob â€“ AI/ML stack
